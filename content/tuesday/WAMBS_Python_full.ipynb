{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "ee2b834e7eac0635",
      "metadata": {
        "id": "ee2b834e7eac0635"
      },
      "source": [
        "# WAMBS Python Tutorial\n",
        "\n",
        "**Authors**: original R Tutorial created by Laurent Smeets and Rens van de Schoot, with updates by Duco Veen; Python translation by Florian Metwaly.\n",
        "\n",
        "In this tutorial you follow the steps of the when-to-Worry-and-How-to-Avoid-the-Misuse-of-Bayesian-Statistics - checklist [(the WAMBS-checklist)](https://www.rensvandeschoot.com/wambs-checklist/)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3138a3fe26f291e3",
      "metadata": {
        "id": "3138a3fe26f291e3"
      },
      "source": [
        "## WAMBS Checklist\n",
        "\n",
        "### *When to worry, and how to Avoid the Misuse of Bayesian Statistics*\n",
        "\n",
        "**To be checked before estimating the model**\n",
        "\n",
        "1. Do you understand the priors?\n",
        "\n",
        "**To be checked after estimation but before inspecting model results**\n",
        "\n",
        "2.    Does the trace-plot exhibit convergence?\n",
        "3.  Does convergence remain after doubling the number of iterations?\n",
        "4.   Does the posterior distribution histogram have enough information?\n",
        "5.   Do the chains exhibit a strong degree of autocorrelation?\n",
        "6.   Do the posterior distributions make substantive sense?\n",
        "\n",
        "**Understanding the exact influence of the priors**\n",
        "\n",
        "7. Do different specifications of the multivariate variance priors influence the results?\n",
        "8.   Is there a notable effect of the prior when compared with non-informative priors?\n",
        "9.   Are the results stable when conducting a sensitivity analysis?\n",
        "10.   Is the Bayesian way of interpreting and reporting model results used?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1736f1595faed30a",
      "metadata": {
        "id": "1736f1595faed30a"
      },
      "source": [
        "## Example Data\n",
        "\n",
        "The data we will be using for this exercise is based on a study about predicting PhD-delays ([Van de Schoot, Yerkes, Mouw and Sonneveld 2013](http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0068839)).  The data can be downloaded [here](https://www.rensvandeschoot.com/wp-content/uploads/2018/10/phd-delays.csv). Among many other questions, the researchers asked the Ph.D. recipients how long it took them to finish their Ph.D. thesis (n=333). It appeared that Ph.D. recipients took an average of 59.8 months (five years and four months) to complete their Ph.D. trajectory. The variable *B3_difference_extra* measures the difference between planned and actual project time in months (mean=9.96, minimum=-31, maximum=91, sd=14.43). For more information on the sample, instruments, methodology and research context we refer the interested reader to the paper.\n",
        "\n",
        "For the current exercise we are interested in the question whether age (M = 30.7, SD = 4.48, min-max = 26-69) of the Ph.D. recipients is related to a delay in their project.\n",
        "\n",
        "The relation between completion time and age is expected to be non-linear. This might be due to that at a certain point in your life (i.e., mid thirties), family life takes up more of your time than when you are in your twenties or when you are older.\n",
        "\n",
        "So, in our model the gap (*B3_difference_extra*) is the dependent variable and age (*E22_Age*) and age$^2$(*E22_Age_Squared *) are the predictors. The data can be found in the file <span style=\"color:red\"> ` phd-delays.csv` </span>.\n",
        "\n",
        "\n",
        "\n",
        "##### _**Question:** Write down the null and alternative hypotheses that represent this question. Which hypothesis do you deem more likely?_\n",
        "\n",
        "<details>\n",
        "<summary>Click to show result</summary>\n",
        "\n",
        "$H_0:$ $age$ is not related to a delay in the PhD projects.\n",
        "\n",
        "$H_1:$ $age$ is related to a delay in the PhD projects.\n",
        "\n",
        "$H_0:$ $age^2$ is not related to a delay in the PhD projects.\n",
        "\n",
        "$H_1:$ $age^2$ is related to a delay in the PhD projects.\n",
        "\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4e12ec28aec8c443",
      "metadata": {
        "id": "4e12ec28aec8c443"
      },
      "source": [
        "## Preparation - Importing and Exploring Data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "58a780e0d887a689",
      "metadata": {
        "id": "58a780e0d887a689"
      },
      "source": [
        "If you are using Google Colab, remove the # from the following line and run the chunk to install the `bambi` library:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d8dcb386db0bcdcf",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-05-20T12:56:07.549991Z",
          "start_time": "2025-05-20T12:56:07.537002Z"
        },
        "id": "d8dcb386db0bcdcf"
      },
      "outputs": [],
      "source": [
        "#!pip install bambi"
      ]
    },
    {
      "cell_type": "code",
      "id": "55a3b009ea8bf1f8",
      "metadata": {
        "id": "55a3b009ea8bf1f8",
        "ExecuteTime": {
          "end_time": "2025-05-23T09:36:51.243801Z",
          "start_time": "2025-05-23T09:36:47.360271Z"
        }
      },
      "source": [
        "import bambi as bmb # BAyesian Model-Building Interface in Python\n",
        "import arviz as az # Exploratory analysis of Bayesian models\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt # plotting\n",
        "\n",
        "SEED = 1337 # set a seed to get consistent results"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "id": "90f3c08b68ce6542",
      "metadata": {
        "id": "90f3c08b68ce6542"
      },
      "source": [
        "You can find the data in the file <span style=\"color:red\"> ` phd-delays.csv` </span>, which contains all variables that you need for this analysis. Although it is a .csv-file, you can directly load it into Python using the following syntax:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "510fc4c857f17d69",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-05-20T12:56:14.641382Z",
          "start_time": "2025-05-20T12:56:14.625609Z"
        },
        "id": "510fc4c857f17d69"
      },
      "outputs": [],
      "source": [
        "dataPHD = pd.read_csv(\"phd-delays.csv\", sep=\";\")\n",
        "dataPHD.columns = [\"diff\", \"child\", \"sex\", \"age\", \"age2\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "784f2dfb727b52e9",
      "metadata": {
        "id": "784f2dfb727b52e9"
      },
      "source": [
        "Alternatively, you can directly download them from GitHub into your Python work space using the following command:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ed681cf24eaaa707",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-05-20T12:56:15.003468Z",
          "start_time": "2025-05-20T12:56:14.663647Z"
        },
        "id": "ed681cf24eaaa707"
      },
      "outputs": [],
      "source": [
        "dataPHD = pd.read_csv(\"https://raw.githubusercontent.com/UtrechtUniversity/BayesianEstimation/main/content/tuesday/phd-delays.csv\", sep=\";\")\n",
        "dataPHD.columns = [\"diff\", \"child\", \"sex\", \"age\", \"age2\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7afc22e5551f793e",
      "metadata": {
        "id": "7afc22e5551f793e"
      },
      "source": [
        "GitHub is a platform that allows researchers and developers to share code, software and research and to collaborate on projects (see https://github.com/)\n",
        "\n",
        "Once you have loaded in your data, it is advisable to check whether your data import worked well. Therefore, first have a look at the summary statistics of your data. you can do this by using the  `DataFrame.describe()` function."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e329192a1305bd61",
      "metadata": {
        "id": "e329192a1305bd61"
      },
      "source": [
        "##### _**Question:** Have all your data been loaded in correctly? That is, do all data points substantively make sense? If you are unsure, go back to the .csv-file to inspect the raw data._"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6464646e4f9d21bb",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-05-20T12:56:15.055415Z",
          "start_time": "2025-05-20T12:56:15.023053Z"
        },
        "id": "6464646e4f9d21bb"
      },
      "outputs": [],
      "source": [
        "dataPHD.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9086b07fb6925197",
      "metadata": {
        "id": "9086b07fb6925197"
      },
      "source": [
        "_The descriptive statistics make sense:_\n",
        "\n",
        "_diff: Mean (9.97), SE (0.79)_\n",
        "\n",
        "_age: Mean (31.68), SE (0.38)_\n",
        "\n",
        "_age2: Mean (1050.22), SE (35.97)_\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2f7a7f7de911bd30",
      "metadata": {
        "id": "2f7a7f7de911bd30"
      },
      "source": [
        "##   **Step 1: Do you understand the priors?**\n",
        "\n",
        "\n",
        "### 1.Do you understand the priors?\n",
        "\n",
        "\n",
        "Before actually looking at the data we first need to think about the prior distributions and hyperparameters for our model. For the current model, there are four priors for the following four parameters:\n",
        "\n",
        "- the intercept\n",
        "- the two regression parameters ($\\beta_1$ for the relation with AGE and $\\beta_2$ for the relation with AGE2)\n",
        "- the residual variance ($\\sigma^2_{\\epsilon}$)\n",
        "\n",
        "We first need to determine which distribution to use for the priors. Let&#39;s use for the\n",
        "\n",
        "- intercept a normal prior with $\\mathcal{N}(\\mu_0, \\sigma^{2}_{0})$, where $\\mu_0$ is the prior mean of the distribution and $\\sigma^{2}_{0}$ is the variance parameter\n",
        "- $\\beta_1$ a normal prior with $\\mathcal{N}(\\mu_1, \\sigma^{2}_{1})$\n",
        "- $\\beta_2$ a normal prior with $\\mathcal{N}(\\mu_2, \\sigma^{2}_{2})$\n",
        "- $\\sigma^2_{\\epsilon}$ an Inverse Gamma distribution with $IG(\\kappa_0,\\theta_0)$, where $\\kappa_0$ is the shape parameter of the distribution and $\\theta_0$ the rate parameter\n",
        "\n",
        "Next, we need to specify actual values for the hyperparameters of the prior distributions. Let&#39;s say we found a previous study and based on this study the following hyperparameters can be specified:\n",
        "\n",
        "- intercept $\\sim \\mathcal{N}(-35, 20)$\n",
        "- $\\beta_1 \\sim \\mathcal{N}(.8, 5)$\n",
        "- $\\beta_2 \\sim \\mathcal{N}(0, 10)$\n",
        "- $\\sigma^2_{\\epsilon} \\sim IG(.5, .5)$ This is an uninformative prior for the residual variance, which has been found to perform well in simulation studies.\n",
        "\n",
        "It is a good idea to plot these distributions to see how they look. To do so, one easy way is to sample a lot of values from one of these distributions and make a density plot out of it, see the code below. Replace the 'XX' with the values of the hyperparameters. *Note*:The scale parameter uses the standard deviation, not the variance. Therefore we use `np.sqrt(XX)`."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fa737db381eded3e",
      "metadata": {
        "id": "fa737db381eded3e"
      },
      "source": [
        "<pre><code>\n",
        "n = 100_000\n",
        "\n",
        "prior_intercept = np.random.normal(loc=XX, scale=np.sqrt(XX), size=n)\n",
        "effect_age = np.random.normal(loc=XX, scale=np.sqrt(XX), size=n)\n",
        "effect_age2 = np.random.normal(loc=XX, scale=np.sqrt(XX), size=n)\n",
        "\n",
        "fig, axs = plt.subplots(2, 2, figsize=(10, 8))\n",
        "\n",
        "az.plot_dist(prior_intercept, ax=axs[0, 0])\n",
        "axs[0, 0].set_title(\"Prior Intercept\")\n",
        "\n",
        "az.plot_dist(effect_age, ax=axs[0, 1])\n",
        "axs[0, 1].set_title(\"Effect Age\")\n",
        "\n",
        "az.plot_dist(effect_age2, ax=axs[1, 0])\n",
        "axs[1, 0].set_title(\"Effect Age²\")\n",
        "\n",
        "axs[1, 1].axis(\"off\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "</code></pre>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<details>\n",
        "<summary>Click to show result</summary>\n",
        "\n",
        "You can copy-paste the code below in a new code block and run the block to see the result\n",
        "\n",
        "```python\n",
        "n = 100_000\n",
        "prior_intercept = np.random.normal(loc=-35, scale=np.sqrt(20), size=n)\n",
        "effect_age = np.random.normal(loc=0.8, scale=np.sqrt(5), size=n)\n",
        "effect_age2 = np.random.normal(loc=0, scale=np.sqrt(10), size=n)\n",
        "\n",
        "fig, axs = plt.subplots(2, 2, figsize=(10, 8))\n",
        "\n",
        "az.plot_dist(prior_intercept, ax=axs[0, 0])\n",
        "axs[0, 0].set_title(\"Prior Intercept\")\n",
        "\n",
        "az.plot_dist(effect_age, ax=axs[0, 1])\n",
        "axs[0, 1].set_title(\"Effect Age\")\n",
        "\n",
        "az.plot_dist(effect_age2, ax=axs[1, 0])\n",
        "axs[1, 0].set_title(\"Effect Age²\")\n",
        "\n",
        "axs[1, 1].axis(\"off\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "</details>"
      ],
      "metadata": {
        "id": "6YyUiI0GX8_v"
      },
      "id": "6YyUiI0GX8_v"
    },
    {
      "cell_type": "markdown",
      "id": "cd2283ea8995dab3",
      "metadata": {
        "id": "cd2283ea8995dab3"
      },
      "source": [
        "We can also plot what the expected delay would be given these priors. With these priors the regression formula would be: $delay=-35+ .8*age + 0*age^2$. These are just the means of the priors and do not yet qualify the different levels of uncertainty. Replace the 'XX' in the following code with the prior means."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<pre><code>\n",
        "years = np.arange(20, 81)  # 20 to 80 inclusive\n",
        "delay = XX + XX * years + XX * years**2\n",
        "\n",
        "plt.plot(years, delay, linestyle='-')\n",
        "plt.xlabel(\"Years\")\n",
        "plt.ylabel(\"Delay\")\n",
        "plt.title(\"Delay vs Years\")\n",
        "plt.show()\n",
        "</code></pre>"
      ],
      "metadata": {
        "id": "Ts8R2FX-ZmnO"
      },
      "id": "Ts8R2FX-ZmnO"
    },
    {
      "cell_type": "markdown",
      "source": [
        "<details>\n",
        "<summary>Click to show result</summary>\n",
        "\n",
        "You can copy-paste the code below in a new code block and run the block to see the result\n",
        "\n",
        "```python\n",
        "years = np.arange(20, 81)  # 20 to 80 inclusive\n",
        "delay = -35 + 0.8 * years + 0 * years**2\n",
        "\n",
        "plt.plot(years, delay, linestyle='-')\n",
        "plt.xlabel(\"Years\")\n",
        "plt.ylabel(\"Delay\")\n",
        "plt.title(\"Delay vs Years\")\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "<\\details>"
      ],
      "metadata": {
        "id": "bxR0m4J_ZRiC"
      },
      "id": "bxR0m4J_ZRiC"
    },
    {
      "cell_type": "markdown",
      "id": "8befc799b447dace",
      "metadata": {
        "id": "8befc799b447dace"
      },
      "source": [
        "## **Step 2: Run the model and check for convergence**\n",
        "\n",
        "\n",
        "To run a multiple regression with `bambi`, you first specify the model, then fit the model and finally acquire the summary (similar to the frequentist model using  `lm()`). The model is specified as follows:\n",
        "\n",
        "\n",
        "1.  A dependent variable we want to predict.\n",
        "2.  A \"~\", that we use to indicate that we now provide the other variables of interest\n",
        "    (comparable to the '=' of the regression equation).\n",
        "3.  The different independent variables separated by the summation symbol '+'.\n",
        "4.  Finally, we insert that the dependent variable has a variance and that we\n",
        "    want an intercept.\n",
        "5. We do set a seed to make the results exactly reproducible.\n",
        "6. To specify priors, we use the `bmb.Prior` function. Be careful, `bambi` uses standard deviations instead of variances in the normal distribution. The standard deviations is the square root of the variance, so a variance of 5 corresponds to a standard deviation of 2.24 and a variance of 10 corresponds to a standard deviation of 3.16.\n",
        "\n",
        "\n",
        "There are many other options we can select, such as the number of chains, how many iterations we want, and how long of a warm-up phase we want, but we will just use the defaults for now.\n",
        "\n",
        "For more information on the basics of `bambi`, see the [website and vignettes](https://bambinos.github.io/bambi/).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "63d3ebe4ffd2cedc",
      "metadata": {
        "id": "63d3ebe4ffd2cedc"
      },
      "source": [
        "### 2. Does the trace plot exhibit convergence?\n",
        "\n",
        "First we run the analysis with only a short burn-in period of 250 samples and then we take another 250 samples.\n",
        "\n",
        "The following code is how to specify the regression model:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4da27ec61116d744",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-05-20T12:56:15.777633Z",
          "start_time": "2025-05-20T12:56:15.768989Z"
        },
        "id": "4da27ec61116d744"
      },
      "outputs": [],
      "source": [
        "# Specify Priors\n",
        "prior_age = bmb.Prior(\"Normal\", mu=.8, sigma=2.24)\n",
        "prior_age2 = bmb.Prior(\"Normal\", mu=.0, sigma=3.16)\n",
        "prior_int = bmb.Prior(\"Normal\", mu=-35, sigma=4.47)\n",
        "prior_sigma = bmb.Prior(\"InverseGamma\", alpha=.5, beta=.5)\n",
        "\n",
        "priors_inf = {\"Intercept\": prior_int, \"age\": prior_age, \"age2\": prior_age2, \"sigma\": prior_sigma}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "629010c2ca29db2",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-05-20T12:57:04.302474Z",
          "start_time": "2025-05-20T12:56:15.841223Z"
        },
        "id": "629010c2ca29db2"
      },
      "outputs": [],
      "source": [
        "# Specify Model\n",
        "model_few_samples = bmb.Model(\"diff ~ age + age2\",\n",
        "                  data=dataPHD,\n",
        "                  priors=priors_inf,\n",
        "                  center_predictors=False)\n",
        "\n",
        "# Run Model\n",
        "## tune sets the burn-in samples\n",
        "## draws sets the post burn-in draws\n",
        "fit_few_samples = model_few_samples.fit(random_seed=SEED,\n",
        "                tune=250,\n",
        "                draws=250)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "96720203491c68e9",
      "metadata": {
        "id": "96720203491c68e9"
      },
      "source": [
        "Now we can plot the trace plots."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e502a41b8eda531b",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-05-20T12:57:06.763010Z",
          "start_time": "2025-05-20T12:57:04.465568Z"
        },
        "id": "e502a41b8eda531b"
      },
      "outputs": [],
      "source": [
        "# Extract the posterior samples\n",
        "## NOTE: The burn in phase is automatically discarded by bambi, so those draws are not shown in the plots\n",
        "samples = fit_few_samples.posterior\n",
        "\n",
        "# Extract variable names\n",
        "parameters = list(samples.keys())\n",
        "\n",
        "# Plot each parameters trace plot\n",
        "for param in parameters:\n",
        "    values = samples[param]\n",
        "    plt.figure(figsize=(10, 3))\n",
        "    for chain in range(values.shape[0]):\n",
        "        plt.plot(values[chain].values, label=f\"Chain {chain}\")\n",
        "    plt.title(f\"Trace plot for {param}\")\n",
        "    plt.xlabel(\"Iteration\")\n",
        "    plt.ylabel(\"Value\")\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a625128bcc53d94b",
      "metadata": {
        "id": "a625128bcc53d94b"
      },
      "source": [
        "Alternatively, you can simply make use of the `arviz` library to get the trace plots (and other convergence statistics)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "50e524a111a13d2a",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-05-20T12:57:08.129978Z",
          "start_time": "2025-05-20T12:57:06.811440Z"
        },
        "id": "50e524a111a13d2a"
      },
      "outputs": [],
      "source": [
        "az.plot_trace(fit_few_samples)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "41146f4a770885ba",
      "metadata": {
        "id": "41146f4a770885ba"
      },
      "source": [
        "It seems like the trace (caterpillar) plots are mostly converged into one each other (we ideally want one fat caterpillar), though the caterpillar could be 'fatter'. Additionally, we get the marginal posterior distribution for each chain, in which we can also see that the sampled posteriors differ slightly between the chains, indicating that the sampler hasn't fully converged and we want more samples.\n",
        "\n",
        "We can check if the chains have reached convergence by having a look at the convergence diagnostics. Two of these diagnostics of interest include the Gelman and Rubin diagnostic and the Geweke diagnostic.\n",
        "\n",
        "* The Gelman-Rubin Diagnostic shows the potential scale reduction factor (PSRF) values (using the  within and between chain variability), which all should be close to 1 ($\\geq 1.01$). If they aren't close to 1, you should use more iterations. Note: The Gelman and Rubin diagnostic is also automatically given in the summary provided by `bambi` under the column `r_hat`.\n",
        "* The Geweke diagnostic is not readily availabe in `bambi` but can be calculated manually (if you want; usually $\\hat{R}$ suffices)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "24e8cdf85b6f4927",
      "metadata": {
        "id": "24e8cdf85b6f4927"
      },
      "source": [
        "To obtain the Gelman and Rubin diagnostic use:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1e8f7bf659400f77",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-05-20T12:57:08.295520Z",
          "start_time": "2025-05-20T12:57:08.251869Z"
        },
        "id": "1e8f7bf659400f77"
      },
      "outputs": [],
      "source": [
        "az.rhat(fit_few_samples)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "421660b6f8c2cef9",
      "metadata": {
        "id": "421660b6f8c2cef9"
      },
      "source": [
        "Additionally we can use rank plots. Rank plots are histograms of ranked posterior draws, plotted separately for each chain. The rank plots should show a uniform distribution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3f5271a60a939538",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-05-20T12:57:09.940736Z",
          "start_time": "2025-05-20T12:57:08.419593Z"
        },
        "id": "3f5271a60a939538"
      },
      "outputs": [],
      "source": [
        "az.plot_rank(fit_few_samples)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cd804ea7fc131f07",
      "metadata": {
        "id": "cd804ea7fc131f07"
      },
      "source": [
        "These statistics confirm that the chains have not completely converged (although they are not too problematic either). Therefore (and for the sake of illustration), we run the same analysis with more samples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "82fdba456fbdbe92",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-05-20T12:58:45.893560Z",
          "start_time": "2025-05-20T12:57:10.154547Z"
        },
        "id": "82fdba456fbdbe92"
      },
      "outputs": [],
      "source": [
        "# Specify Model\n",
        "model = bmb.Model(\"diff ~ age + age2\",\n",
        "                  data=dataPHD,\n",
        "                  priors=priors_inf,\n",
        "                  center_predictors=False)\n",
        "\n",
        "# Run Model with more iterations\n",
        "fit = model.fit(random_seed=SEED,\n",
        "                tune=2000,\n",
        "                draws=2000)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9d29b253b82f48f0",
      "metadata": {
        "id": "9d29b253b82f48f0"
      },
      "source": [
        "Obtain the trace plots again."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a50d916cc1466d4b",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-05-20T12:58:47.868905Z",
          "start_time": "2025-05-20T12:58:46.020562Z"
        },
        "id": "a50d916cc1466d4b"
      },
      "outputs": [],
      "source": [
        "az.plot_trace(fit)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "30be349cc4aedfa",
      "metadata": {
        "id": "30be349cc4aedfa"
      },
      "source": [
        "Obtain the Gelman and Rubin diagnostic again."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b1928f228af8f33e",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-05-20T12:58:48.056664Z",
          "start_time": "2025-05-20T12:58:47.991475Z"
        },
        "id": "b1928f228af8f33e"
      },
      "outputs": [],
      "source": [
        "az.rhat(fit)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1623cd956b61ee5b",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-05-20T13:02:50.638524300Z",
          "start_time": "2025-05-20T12:58:48.419991Z"
        },
        "id": "1623cd956b61ee5b"
      },
      "outputs": [],
      "source": [
        "az.plot_rank(fit)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b546d80ddf77e4e8",
      "metadata": {
        "id": "b546d80ddf77e4e8"
      },
      "source": [
        "Now we see that the Gelman and Rubin diagnostic (PRSF) is close to 1 for all parameters and the ranks look uniformly distributed."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ad7a752e4f94e8cd",
      "metadata": {
        "id": "ad7a752e4f94e8cd"
      },
      "source": [
        "### 3. Does convergence remain after doubling the number of iterations?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "16e08a52370b4c44",
      "metadata": {
        "id": "16e08a52370b4c44"
      },
      "source": [
        "As is recommended in the WAMBS checklist, we double the amount of iterations to check for local convergence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2d5be7613a0f4f19",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-05-20T13:01:22.346659Z",
          "start_time": "2025-05-20T12:58:50.647040Z"
        },
        "id": "2d5be7613a0f4f19"
      },
      "outputs": [],
      "source": [
        "# Specify Model\n",
        "model_doubleiter = bmb.Model(\"diff ~ age + age2\",\n",
        "                             data=dataPHD,\n",
        "                             priors=priors_inf,\n",
        "                             center_predictors=False)\n",
        "\n",
        "# Run Model with double more iterations\n",
        "fit_doubleiter = model_doubleiter.fit(random_seed=SEED,\n",
        "                tune=4000,\n",
        "                draws=4000)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "99148c57f6a62bd1",
      "metadata": {
        "id": "99148c57f6a62bd1"
      },
      "source": [
        "You should again have a look at the above-mentioned convergence statistics, but we can also compute the relative bias to inspect if doubling the number of iterations influences the posterior parameter estimates ($bias= 100*\\frac{(model \\; with \\; double \\; iteration \\; - \\; initial \\; converged \\; model )}{initial \\; converged \\; model}$). In order to preserve clarity we  just calculate the bias of the two regression coefficients.\n",
        "\n",
        "You should combine the relative bias in combination with substantive knowledge about the metric of the parameter of interest to determine when levels of relative deviation are negligible or problematic. For example, with a regression coefficient of 0.001, a 5% relative deviation level might not be substantively relevant. However, with an intercept parameter of 50, a 10% relative deviation level might be quite meaningful. The specific level of relative deviation should be interpreted in the substantive context of the model. Some examples of interpretations are:\n",
        "\n",
        "- if relative deviation is &lt; |5|%, then do not worry;\n",
        "- if relative deviation &gt; |5|%, then rerun with 4x the number of iterations."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "78708669b77c9de8",
      "metadata": {
        "id": "78708669b77c9de8"
      },
      "source": [
        "_**Question:** calculate the relative bias. Are you satisfied with number of iterations, or do you want to re-run the model with even more iterations?_\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "69d961857efc8a78",
      "metadata": {
        "id": "69d961857efc8a78"
      },
      "source": [
        "To get the relative bias, save the means of the regression coefficients (posterior means) and other parameters for the two different analyses and compute the bias."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Click here for the solution\n",
        "# Get posterior summaries\n",
        "summary_fit = az.summary(fit)\n",
        "summary_fit_doubleiter = az.summary(fit_doubleiter)\n",
        "\n",
        "# Extract posterior means\n",
        "means_fit = summary_fit[\"mean\"]\n",
        "means_fit_doubleiter = summary_fit_doubleiter[\"mean\"]\n",
        "\n",
        "# Compute % change\n",
        "percent_change = 100 * ((means_fit_doubleiter - means_fit) / means_fit)\n",
        "percent_change_rounded = percent_change.round(4)\n",
        "\n",
        "print(percent_change_rounded)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "y10iiZYRfhUz"
      },
      "id": "y10iiZYRfhUz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "c382a1c1c6e06da7",
      "metadata": {
        "id": "c382a1c1c6e06da7"
      },
      "source": [
        "### 4.   Does the posterior distribution histogram have enough information?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "70f46236cc544eee",
      "metadata": {
        "id": "70f46236cc544eee"
      },
      "source": [
        "By having a look at the posterior distribution histogram `az.plot_posterior(fit, kind=\"hist\", bins=30)`, we can check if it contains enough information."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "63c782440c6eb8a9",
      "metadata": {
        "id": "63c782440c6eb8a9"
      },
      "source": [
        "_**Question:** What can you conclude about the distribution histograms?_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "64bfd6ee92e6793",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-05-20T13:01:23.199866Z",
          "start_time": "2025-05-20T13:01:22.618368Z"
        },
        "id": "64bfd6ee92e6793"
      },
      "outputs": [],
      "source": [
        "az.plot_posterior(fit, kind=\"hist\", bins=30)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ecd0a382",
      "metadata": {
        "id": "ecd0a382"
      },
      "source": [
        "<details>\n",
        "<summary>Click to show interpretation</summary>\n",
        "\n",
        "_The histograms look smooth and have no gaps or other abnormalities. Based on this, adding more iterations is not necessary. However, if you are not satisfied, you can increase the number of iterations again. Posterior distributions do not have to be symmetrical, but in this example they seem to be._\n",
        "\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "38917ebde209c03b",
      "metadata": {
        "id": "38917ebde209c03b"
      },
      "source": [
        "If we compare this with histograms based on the first analysis (with very few iterations), this difference becomes clear:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "10f58315eb9aeca1",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-05-20T13:01:23.827051Z",
          "start_time": "2025-05-20T13:01:23.240607Z"
        },
        "id": "10f58315eb9aeca1"
      },
      "outputs": [],
      "source": [
        "az.plot_posterior(fit_few_samples, kind = \"hist\", bins=30)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9c2fa30241f1e760",
      "metadata": {
        "id": "9c2fa30241f1e760"
      },
      "source": [
        "### 5.   Is the effective sample size sufficient or do the chains exhibit a strong degree of autocorrelation?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "112323f414fc5215",
      "metadata": {
        "id": "112323f414fc5215"
      },
      "source": [
        "The effective sample size (ESS) is a measure of the number of independent samples in a Markov Chain Monte Carlo (MCMC) chain and reflects the efficiency of the algorithm.  It accounts for the autocorrelation in the chains, which can reduce the effective number of samples. A higher ESS indicates that the MCMC chain is more efficient and provides more reliable estimates. `bambi` gives a warning if the number of ESS is too small."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "az.ess(fit)"
      ],
      "metadata": {
        "id": "h_N_GVGDiv_L"
      },
      "id": "h_N_GVGDiv_L",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To investigate the level of autocorrelation use:"
      ],
      "metadata": {
        "id": "5srcSwXZjKcF"
      },
      "id": "5srcSwXZjKcF"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "77ee386089e805f6",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-05-20T13:01:26.549496Z",
          "start_time": "2025-05-20T13:01:24.013021Z"
        },
        "id": "77ee386089e805f6"
      },
      "outputs": [],
      "source": [
        "az.plot_autocorr(fit)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2396591d3b773e94",
      "metadata": {
        "id": "2396591d3b773e94"
      },
      "source": [
        "_**Question:** What can you conclude about these autocorrelation plots?_"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4955d2fab86be510",
      "metadata": {
        "id": "4955d2fab86be510"
      },
      "source": [
        "<details>\n",
        "<summary>Click to show interpretation</summary>\n",
        "\n",
        "These results show that autocorrelation is quite stong after a few lags. This means it is important to make sure we ran the analysis with a lot of samples, because with a high autocorrelation it will take longer until the whole parameter space has been identified. Historically, thinning has been used to reduce autocorrelation in MCMC chains, but this is no longer recommended. Instead, it is better to increase the number of iterations and warmup samples to ensure that the chains are well-mixed and that the effective sample size is sufficient. For more information on autocorrelation check this [paper](https://besjournals.onlinelibrary.wiley.com/doi/full/10.1111/j.2041-210X.2011.00131.x).\n",
        "\n",
        "</details>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7c14f184f2758b4",
      "metadata": {
        "id": "7c14f184f2758b4"
      },
      "source": [
        "### 6.   Do the posterior distributions make substantive sense?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e54c1383abe44ca4",
      "metadata": {
        "id": "e54c1383abe44ca4"
      },
      "source": [
        "We plot the posterior distributions and see 1) if they are unimodel (one peak); 2) if they are clearly centered around one value; 3) if they give a realistic estimate; and 4) if they make substantive sense compared to our prior believes (priors). Here we plot the  posteriors of the regression coefficients. If you want you can also plot the mean and the 95% Posterior HPD Intervals.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "54dc8213eb2e8e8e",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-05-20T13:01:27.252364Z",
          "start_time": "2025-05-20T13:01:26.636665Z"
        },
        "id": "54dc8213eb2e8e8e"
      },
      "outputs": [],
      "source": [
        "az.plot_posterior(fit, hdi_prob=0.95)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3339150c5bdbda2f",
      "metadata": {
        "id": "3339150c5bdbda2f"
      },
      "source": [
        "_**Question:** What is your conclusion; do the posterior distributions make sense?_\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e9dade6206305dce",
      "metadata": {
        "id": "e9dade6206305dce"
      },
      "source": [
        "<details>\n",
        "<summary>Click to show interpretation</summary>\n",
        "\n",
        "_Yes, we see a clear negative intercept, which makes sense since a value of age = 0 for Ph.D is impossible. We also have have plausible ranges of values for the regression coefficients and a positive variance._\n",
        "\n",
        "</details>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In addition, do the posterior predictive distributions make sense substantively and in light of the observed data? For this, we want to look at predictions on the response scale the model makes based on the posterior parameter estimates."
      ],
      "metadata": {
        "id": "dnKBWO7Vjuvc"
      },
      "id": "dnKBWO7Vjuvc"
    },
    {
      "cell_type": "code",
      "source": [
        "idata = model.fit(draws=2000, tune=1000)\n",
        "idata.extend(model.predict(idata, kind=\"response\", inplace=False))\n",
        "\n",
        "az.plot_ppc(idata)"
      ],
      "metadata": {
        "id": "VJbfyHSUkTdR"
      },
      "id": "VJbfyHSUkTdR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "_**Question:** Do the posterior predictive distributions make sense substantively and in light of the observed data?_\n",
        "\n",
        "<details>\n",
        "<summary>Click to show interpretation</summary>\n",
        "This is often the tricky part. Clearly, we see that the model does not perfectly capture the shape of the observed data, which is not symmetrical. Although we could argue that the fit is good enough for our purposes, we could also investigate how to improve the fit by considering alternative likelihood functions, such as a shifted lognormal distribution, or setting a lower bound based on the minimum possible delay (e.g., 2 years, so 12 months). Do note that while these changes will most likely improve the posterior fit, because they better suit the data-generating process, they will also make interpretation more complex.\n",
        "</details>"
      ],
      "metadata": {
        "id": "pOGk7TZulREC"
      },
      "id": "pOGk7TZulREC"
    },
    {
      "cell_type": "markdown",
      "id": "3d4311da19e2fbd8",
      "metadata": {
        "id": "3d4311da19e2fbd8"
      },
      "source": [
        "## **Step 3: Understanding the exact influence of the priors**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "53fd89da5ede72d1",
      "metadata": {
        "id": "53fd89da5ede72d1"
      },
      "source": [
        "First we  check the results of the analysis with the priors we used so far."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "922da3925cb103bc",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-05-20T13:01:27.725136Z",
          "start_time": "2025-05-20T13:01:27.619707Z"
        },
        "id": "922da3925cb103bc"
      },
      "outputs": [],
      "source": [
        "az.summary(fit)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "93d3b023e4352fd0",
      "metadata": {
        "id": "93d3b023e4352fd0"
      },
      "source": [
        "### 7. Do different specifications of the variance priors influence the results?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5255a790a23522f3",
      "metadata": {
        "id": "5255a790a23522f3"
      },
      "source": [
        "So far we have used the $\\sigma^2_{\\epsilon} \\sim IG(.5, .5)$ prior, but we can also use the $\\sigma^2_{\\epsilon} \\sim IG(.01, .01)$ prior and see if doing so makes a difference. To quantify this difference we again calculate a relative bias."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3dc5ff16f0c225b8",
      "metadata": {
        "id": "3dc5ff16f0c225b8"
      },
      "source": [
        "_**Question:** Are the results robust for different specifications of the prior on the residual variance?_\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e7669de7f0b18225",
      "metadata": {
        "id": "e7669de7f0b18225"
      },
      "source": [
        "| Parameters | Estimate with $\\sigma^2_{\\epsilon} \\sim IG(.01, .01)$ | Estimate with $\\sigma^2_{\\epsilon} \\sim IG(.5, .5)$ | Bias |\n",
        "| --- | --- | --- | --- |\n",
        "| Intercept | |  | |\n",
        "| Age       | |  | |\n",
        "| Age2      | |  | |\n",
        "| Residual variance |  |  |  |\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ea8288c402d60b21",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-05-20T13:02:49.801071Z",
          "start_time": "2025-05-20T13:01:27.986066Z"
        },
        "id": "ea8288c402d60b21"
      },
      "outputs": [],
      "source": [
        "# Specify Priors\n",
        "prior_age = bmb.Prior(\"Normal\", mu=.8, sigma=2.24)\n",
        "prior_age2 = bmb.Prior(\"Normal\", mu=.0, sigma=3.16)\n",
        "prior_int = bmb.Prior(\"Normal\", mu=-35, sigma=4.47)\n",
        "prior_sigma = bmb.Prior(\"InverseGamma\", alpha=.01, beta=.01)\n",
        "\n",
        "priors_inf2 = {\"Intercept\": prior_int, \"age\": prior_age, \"age2\": prior_age2, \"sigma\": prior_sigma}\n",
        "\n",
        "# Specify Model\n",
        "model_difIG = bmb.Model(\"diff ~ age + age2\",\n",
        "                  data=dataPHD,\n",
        "                  priors=priors_inf2,\n",
        "                  center_predictors=False)\n",
        "\n",
        "# Run Model\n",
        "## tune sets the burn-in samples\n",
        "## draws sets the post burn-in draws\n",
        "fit_difIG = model_difIG.fit(random_seed=SEED,\n",
        "                tune=2000,\n",
        "                draws=4000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d9ae69625ab08475",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-05-20T13:03:34.014372Z",
          "start_time": "2025-05-20T13:03:33.943590Z"
        },
        "id": "d9ae69625ab08475"
      },
      "outputs": [],
      "source": [
        "az.summary(fit_difIG)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Click to show the code\n",
        "\n",
        "# Get posterior summary of the different IG prior model\n",
        "summary_fit_difIG = az.summary(fit_difIG)\n",
        "\n",
        "# Extract posterior mean\n",
        "means_fit_difIG = summary_fit_difIG[\"mean\"]\n",
        "\n",
        "# Compute % change\n",
        "percent_change_difIG = 100 * ((means_fit_difIG - means_fit) / means_fit)\n",
        "percent_change_rounded_difIG = percent_change_difIG.round(4)\n",
        "\n",
        "#print(percent_change_rounded_difIG)\n",
        "\n",
        "tab = pd.DataFrame({\n",
        "    \"Parameters\" : [\"Intercept\", \"Age\", \"Age2\", \"Residual variance\"],\n",
        "    \"Estimates with IG(0.5,0.5) prior\" : means_fit.round(3),\n",
        "    \"Estimate with IG(0.01,0.01) prior\" : means_fit_difIG.round(3),\n",
        "    \"Bias (%)\" : percent_change_rounded_difIG.round(3)\n",
        "})\n",
        "tab.style.hide(axis=\"index\").format({\n",
        "    \"Estimates with IG(0.5,0.5) prior\": lambda x: f\"{x:.3f}\",\n",
        "    \"Estimate with IG(0.01,0.01) prior\": lambda x: f\"{x:.3f}\",\n",
        "    \"Bias (%)\": lambda x: f\"{x:.3f}\"})\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "LHD97IW7mabu"
      },
      "id": "LHD97IW7mabu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<details>\n",
        "<summary>Click to show interpretation</summary>\n",
        "\n",
        "Yes, the results are robust, because there is only a really small amount of relative bias for the residual variance.\n",
        "\n",
        "</details>"
      ],
      "metadata": {
        "id": "VkzlQpY9qnJR"
      },
      "id": "VkzlQpY9qnJR"
    },
    {
      "cell_type": "markdown",
      "id": "34818bdc01d40aaa",
      "metadata": {
        "id": "34818bdc01d40aaa"
      },
      "source": [
        "### 8.   Is there a notable effect of the prior when compared with non-informative priors?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c4ff02b41044dd1d",
      "metadata": {
        "id": "c4ff02b41044dd1d"
      },
      "source": [
        "The default `bambi` priors are non-informative, so we can run the analysis without any specified priors and compare them to the model we have run thus far, using the relative bias, to see if there is a large influence of the priors.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "95f5f614d58096fa",
      "metadata": {
        "id": "95f5f614d58096fa"
      },
      "source": [
        "_**Question**: What is your conclusion about the influence of the priors on the posterior results?_"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e6ff11f1ced87cf0",
      "metadata": {
        "id": "e6ff11f1ced87cf0"
      },
      "source": [
        "| Parameters | Estimates with default priors | Estimate with informative priors | Bias|\n",
        "| --- | --- | --- | --- |\n",
        "| Intercept |  |      |  |\n",
        "| Age |  |  | |\n",
        "| Age2 |  |  ||\n",
        "| Residual variance | |  |  |"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The default priors in `bambi` are internally scaled to be 'generic weakly informative'. From the manual we learn that:\n",
        "\n",
        "*By default, Bambi will intelligently generate weakly informative priors for all model terms, by loosely scaling them to the observed data. Currently, Bambi uses a methodology very similar to the one described in the documentation of the R package rstanarm. While the default priors will behave well in most typical settings, there are many cases where an analyst will want to specify their own priors–and in general, when informative priors are available, it’s a good idea to use them.*\n",
        "\n",
        "The details on this scaling can be found here: https://mc-stan.org/rstanarm/articles/priors.html#default-weakly-informative-prior-distributions-1"
      ],
      "metadata": {
        "id": "YUnvoCDwt2v_"
      },
      "id": "YUnvoCDwt2v_"
    },
    {
      "cell_type": "markdown",
      "id": "85acca1d0f8d1c16",
      "metadata": {
        "id": "85acca1d0f8d1c16"
      },
      "source": [
        "We can run the model without our priors and check if doing so strongly influences the results. With this fitted model, we can also see what the default priors look like in our case."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ec03421aa182565f",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-05-20T13:24:15.574840Z",
          "start_time": "2025-05-20T13:21:37.688113Z"
        },
        "id": "ec03421aa182565f"
      },
      "outputs": [],
      "source": [
        "# Specify Model\n",
        "model_default = bmb.Model(\"diff ~ age + age2\",\n",
        "                  data=dataPHD,\n",
        "                  center_predictors=False)\n",
        "\n",
        "# Run Model\n",
        "## tune sets the burn-in samples\n",
        "## draws sets the post burn-in draws\n",
        "fit_default = model_default.fit(random_seed=SEED,\n",
        "                tune=2000,\n",
        "                draws=2000)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To see the priors used in the model -- in this case the default priors -- we can use: `model.plot_priors()`"
      ],
      "metadata": {
        "id": "r9kLbUJKrdwh"
      },
      "id": "r9kLbUJKrdwh"
    },
    {
      "cell_type": "code",
      "source": [
        "model_default.plot_priors()"
      ],
      "metadata": {
        "id": "xYjCrb5drc3S"
      },
      "id": "xYjCrb5drc3S",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Or we simply print the model to see the priors that were used."
      ],
      "metadata": {
        "id": "lpKex4qlswiG"
      },
      "id": "lpKex4qlswiG"
    },
    {
      "cell_type": "code",
      "source": [
        "model_default"
      ],
      "metadata": {
        "id": "hyBmPnvaso0-"
      },
      "id": "hyBmPnvaso0-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ac0578712c407981",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-05-20T13:27:23.573861Z",
          "start_time": "2025-05-20T13:27:23.401200Z"
        },
        "id": "ac0578712c407981"
      },
      "outputs": [],
      "source": [
        "az.summary(fit_default)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "_**Question**: What is your conclusion on the influence of the informative versus the default priors?_"
      ],
      "metadata": {
        "id": "Dn6dT7kXu_Lf"
      },
      "id": "Dn6dT7kXu_Lf"
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Click to show the code\n",
        "\n",
        "# Get posterior summary of the default prior model\n",
        "summary_fit_default = az.summary(fit_default)\n",
        "\n",
        "# Extract posterior mean\n",
        "means_fit_default = summary_fit_default[\"mean\"]\n",
        "\n",
        "# Compute % change\n",
        "percent_change_default = 100 * ((means_fit_default - means_fit) / means_fit)\n",
        "percent_change_rounded_default = percent_change_default.round(4)\n",
        "\n",
        "#print(percent_change_rounded_default)\n",
        "\n",
        "tab = pd.DataFrame({\n",
        "    \"Parameters\" : [\"Intercept\", \"Age\", \"Age2\", \"Residual variance\"],\n",
        "    \"Estimates with informative priors\" : means_fit.round(3),\n",
        "    \"Estimate with default priors\" : means_fit_default.round(3),\n",
        "    \"Bias (%)\" : percent_change_rounded_default.round(3)\n",
        "})\n",
        "tab.style.hide(axis=\"index\").format({\n",
        "    \"Estimates with informative priors\": lambda x: f\"{x:.3f}\",\n",
        "    \"Estimate with default priors\": lambda x: f\"{x:.3f}\",\n",
        "    \"Bias (%)\": lambda x: f\"{x:.3f}\"})\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "CqL6wpXHvJAE"
      },
      "id": "CqL6wpXHvJAE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "cf2b9ce1",
      "metadata": {
        "id": "cf2b9ce1"
      },
      "source": [
        "<details>\n",
        "<summary>Click to show interpretation</summary>\n",
        "\n",
        "_The informative priors have quite some influence (up to 25%) on the posterior results of the intercept and regression coefficients. This is not a bad thing, just important to keep in mind._\n",
        "\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e2d89da4018d9b78",
      "metadata": {
        "id": "e2d89da4018d9b78"
      },
      "source": [
        "_**Question**: Which results do you use to draw conclusion on?_"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f7d63ae02c54ad6f",
      "metadata": {
        "id": "f7d63ae02c54ad6f"
      },
      "source": [
        "<details>\n",
        "<summary>Click to show interpretation</summary>\n",
        "\n",
        "_This really depends on where the priors come from. If for example your informative priors come from a reliable source, you should use them. The most important thing is that you choose your priors accurately, and have good arguments to use them. If not, you shouldn&#39;t use really informative priors and use the results based on the non-informative priors._\n",
        "\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "db6b63e672744e26",
      "metadata": {
        "id": "db6b63e672744e26"
      },
      "source": [
        "### 9.   Are the results stable from a sensitivity analysis?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c005d4222c65f6a3",
      "metadata": {
        "id": "c005d4222c65f6a3"
      },
      "source": [
        "If you still have time left, you can adjust the hyperparameters of the priors upward and downward and re-estimating the model with these varied priors to check for robustness.\n",
        "\n",
        "From the original paper:\n",
        "\n",
        "> \"If informative or weakly-informative priors are used, then we suggest running a sensitivity analysis of these priors. When subjective priors are in place, then there might be a discrepancy between results using different subjective prior settings. A sensitivity analysis for priors would entail adjusting the entire prior distribution (i.e., using a completely different prior distribution than before) or adjusting hyperparameters upward and downward and re-estimating the model with these varied priors. Several different hyperparameter specifications can be made in a sensitivity analysis, and results obtained will point toward the impact of small fluctuations in hyperparameter values. [...] The purpose of this sensitivity analysis is to assess how much of an impact the location of the mean hyperparameter for the prior has on the posterior. [...] Upon receiving results from the sensitivity analysis, assess the impact that fluctuations in the hyperparameter values have on the substantive conclusions. Results may be stable across the sensitivity analysis, or they may be highly instable based on substantive conclusions. Whatever the finding, this information is important to report in the results and discussion sections of a paper. We should also reiterate here that original priors should not be modified, despite the results obtained.\"\n",
        "\n",
        "\n",
        "For more information on this topic, please also refer to this [paper](http://psycnet.apa.org/record/2017-52406-001).\n",
        "\n",
        "In addition to sensitivity analyses based on prior settings, you may also include sensitivity checks based on the model structure, such as using a different likelihood function (e.g., the shifted log-normal distribution) or the inclusion of additional predictors or interactions.  "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d05f664ef073af20",
      "metadata": {
        "id": "d05f664ef073af20"
      },
      "source": [
        "### 10.   Is the Bayesian way of interpreting and reporting model results used?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2b332801171d912f",
      "metadata": {
        "id": "2b332801171d912f"
      },
      "source": [
        "For a summary on how to interpret and report models, please refer to https://www.rensvandeschoot.com/bayesian-analyses-where-to-start-and-what-to-report/\n",
        "\n",
        "Aspects to report:\n",
        "\n",
        "1. estimates + credible intervals: make use of the fact that you have actual indication of uncertainty of the model parameters (not just hypothetical). Also add plots to ease interpretability!\n",
        "2. program and packages used\n",
        "3. discussion of priors (justify choices, and report sensitivity)\n",
        "4. discussion of settings (number of chains, number of interaction, warmup, seeds etc.)\n",
        "5. discussion of sampling diagnostics (e.g., $\\hat{R}$, effective sample size)\n",
        "6. perhaps: model fit or model comparison metrics (e.g., loo, Bayes factors)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ca505a8fd6b56114",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-05-20T13:02:50.622810100Z",
          "start_time": "2025-05-20T12:48:09.248584Z"
        },
        "id": "ca505a8fd6b56114"
      },
      "outputs": [],
      "source": [
        "az.summary(fit)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9d71042673a07af5",
      "metadata": {
        "id": "9d71042673a07af5"
      },
      "source": [
        "In the current model we see that:\n",
        "\n",
        "*  The estimate for the intercept is -36.26 [-43.97;-28.45]\n",
        "*  The estimate for the effect of $age$  is 2.14 [1.77;2.53]\n",
        "*  The estimate for the effect of $age^2$  is -0.02 [-0.03;-0.02]\n",
        "\n",
        "\n",
        "We can see that none of 95% Posterior HPD Intervals for these effects include zero, which means we are can be quite certain that all of the effects are different from 0.\n",
        "\n",
        "\n",
        "We used the `bambi` package to fit Bayesian multilevel models in Python. The model was run with 4 chains, each with 4000 iterations and a warmup of 2000 iterations (total post-warmup $N=8000$). The model diagnostics indicated good convergence (largest $\\hat{R}=1$ for the residual variance and sufficient effective sample sizes for all parameters. The smallest $\\hat{N}_{\\text{eff}} = 1234.0$ for the regression coefficient of age (ratio = 0.15), indicating that there is some autocorrelation in the chains.\n",
        "\n",
        "We used an linear regression model with a Gaussian likelihood and informative priors derived from (fictitious) previous studies on the intercept $\\sim \\mathcal{N}(-35, 20)$, the coefficient of age in years $\\beta_1 \\sim \\mathcal{N}(.8, 5)$, the coefficient of squared age $\\beta_2 \\sim \\mathcal{N}(0, 10)$, as we expected a non-linear effect of age, and the residual variance $\\sigma \\sim IG(.5, .5)$. This is an uninformative prior for the residual variance, which has been found to perform well in simulation studies.\n",
        "Sensitivity checks on the priors and model specification indicated that the conclusions are robust to different prior settings for the variance (i.e., $\\sigma \\sim IG(.01, .01)$. However, result do differ when using diffuse default priors, which results in a lower intercept and stronger effects of age, though the pattern remains the same.\n",
        "  \n",
        "\n",
        "Remember how we plotted the relation between delay and years based on the prior information? Now, do the same with the posterior estimates.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cce5a0b4d1d901ac",
      "metadata": {
        "id": "cce5a0b4d1d901ac"
      },
      "outputs": [],
      "source": [
        "years = np.arange(20, 81)  # 20 to 80 inclusive\n",
        "delay = -35 + 2.13 * years - 0.02 * years**2\n",
        "\n",
        "plt.plot(years, delay, linestyle='-')\n",
        "plt.xlabel(\"Years\")\n",
        "plt.ylabel(\"Delay\")\n",
        "plt.title(\"Delay vs Years\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Or, more Bayesian by including the credible interval to show the uncertainty in the predicted values."
      ],
      "metadata": {
        "id": "lJHZ3Kc19tqH"
      },
      "id": "lJHZ3Kc19tqH"
    },
    {
      "cell_type": "code",
      "source": [
        "age_range = np.linspace(20, 80, 100)\n",
        "newdata = pd.DataFrame({\n",
        "    \"age\": age_range,\n",
        "    \"age2\": age_range**2\n",
        "})"
      ],
      "metadata": {
        "id": "rxmLbSHX6Sos"
      },
      "id": "rxmLbSHX6Sos",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "idata_pp = model.predict(idata, data=newdata, kind=\"response\", inplace=False)\n",
        "idata_pp.posterior_predictive\n",
        "yrep = idata_pp.posterior_predictive[\"diff\"].values  # shape: (draws, observations)\n",
        "yrep = yrep.reshape(-1, yrep.shape[-1])"
      ],
      "metadata": {
        "id": "A2aO4DBb8Tmp"
      },
      "id": "A2aO4DBb8Tmp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "mean = yrep.mean(axis=0)\n",
        "lower = np.percentile(yrep, 2.5, axis=0)\n",
        "upper = np.percentile(yrep, 97.5, axis=0)\n",
        "\n",
        "newdata[\"mean\"] = mean\n",
        "newdata[\"lower\"] = lower\n",
        "newdata[\"upper\"] = upper\n"
      ],
      "metadata": {
        "id": "AEqGlr-r8rYt"
      },
      "id": "AEqGlr-r8rYt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(8, 6))\n",
        "\n",
        "# Line for mean prediction\n",
        "plt.plot(newdata[\"age\"], newdata[\"mean\"], color=\"blue\", label=\"Mean Prediction\")\n",
        "\n",
        "# Shaded area for 95% credible interval\n",
        "plt.fill_between(newdata[\"age\"], newdata[\"lower\"], newdata[\"upper\"], color=\"blue\", alpha=0.3)\n",
        "\n",
        "# Reference line\n",
        "plt.axhline(0, color=\"gray\", linestyle=\"--\")\n",
        "\n",
        "# Labels and title\n",
        "plt.title(\"Predicted delay of PhD degree by age\", fontsize=14)\n",
        "plt.xlabel(\"Age (years)\", fontsize=12)\n",
        "plt.ylabel(\"Delay (months)\", fontsize=12)\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "EsR73nd49Nd-"
      },
      "id": "EsR73nd49Nd-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "e8d478625357fcb3",
      "metadata": {
        "id": "e8d478625357fcb3"
      },
      "source": [
        "**References**\n",
        "\n",
        "\n",
        "Depaoli, S., &amp; Van de Schoot, R. (2017). Improving transparency and replication in Bayesian statistics: The WAMBS-Checklist. _Psychological Methods_, _22_(2), 240.\n",
        "\n",
        "Link, W. A., & Eaton, M. J. (2012). On thinning of chains in MCMC. _Methods in ecology and evolution_, _3_(1), 112-115.\n",
        "\n",
        "van Erp, S., Mulder, J., & Oberski, D. L. (2017). Prior sensitivity analysis in default Bayesian structural equation modeling.\n",
        "\n",
        "Van de Schoot, R., &amp; Depaoli, S. (2014). Bayesian analyses: Where to start and what to report. _European Health Psychologist_, _16_(2), 75-84."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "14cd12ff70a79131",
      "metadata": {
        "id": "14cd12ff70a79131"
      },
      "source": [
        "## Original Computing Environment"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install session_info"
      ],
      "metadata": {
        "id": "ea0xoVlc9-k7"
      },
      "id": "ea0xoVlc9-k7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "id": "31b6a688b0dc1337",
      "metadata": {
        "id": "31b6a688b0dc1337",
        "ExecuteTime": {
          "end_time": "2025-06-03T22:05:42.889558Z",
          "start_time": "2025-06-03T22:05:40.964302Z"
        }
      },
      "source": [
        "import session_info\n",
        "session_info.show()"
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "BayesEst",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.3"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}